{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mEGUK7rYoRC"
   },
   "source": [
    "# Take-Home Challenge\n",
    "\n",
    "### Guidelines\n",
    "* We expect that the test should take around 4 hours to do. However, we strongly advise you to carefully read this assignment, think about approaches and try to understand the data before diving into the questions. You are free to spend as much time on it as you want, in the timeframe given by our recruiter.\n",
    "* In case of using this Google Colab, you'll need to upload the files on google drive folder given to you (*listing.html, properties.csv*) running the cell below. \n",
    "* If you want to use some python packages that are not yet installed on this notebook, use !pip install package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LT_oz7zmYwoL"
   },
   "source": [
    "# Data Extraction (CSS + REGEX)\n",
    "\n",
    "Casafari tracks the entire real estate market by aggregating properties from thousands of different websites. The first step of this process is to collect all the relevant information using web crawlers. This task will give a brief overview of how this extraction is made. \n",
    "\n",
    "The task consists of 3 parts, which will evaluate your skills in CSS3 selectors and regular expressions knowledge, which are essential to data extraction processes. We believe that even if you do not have previous knowledge of CSS, HTML and REGEX, you should be able to complete this task in less than a hour. There are many tutorials and informations on how to use CSS3 selectors and regular expressions to extract data. Do not be afraid to google it! This task is also a evaluation of your learning capabilities.\n",
    "\n",
    "The normal questions already have some examples and can be solved only by filling the CSS3 selectors or the regular expressions in the given space. You can check if you have the correct results by running the pre-made script after it. However, if you feel comfortable, you can use another python package and rewrite the script in a similar way to extract the data.\n",
    "\n",
    "For the extra challenges, you'll need to construct the scripts from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D58kOtOXjrR7"
   },
   "source": [
    "__(1)__ For the following task, use the _listing.html_ file, which represents a listings for a property. Open the HTML file on your browser, investigate it with the Inspect tool, view the source code and explore it. \n",
    "After that, fill the CSS3 selectors in the following script to extract the following information about this property:\n",
    "\n",
    "* Number of bathrooms\n",
    "* Number of bedrooms\n",
    "* Living Area\n",
    "* Energy Rating\n",
    "* Description\n",
    "* Agent Name\n",
    "* Extract the location of the property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9000,
     "status": "ok",
     "timestamp": 1559396967613,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "_ubceG6_rEek",
    "outputId": "7e65fd53-1e43-434c-885c-d6e3f1c098d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install lxml\n",
    "!pip install cssselect\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html, etree\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cqba2Ye43Hyy"
   },
   "outputs": [],
   "source": [
    "# EXAMPLE SELECTOR TO EXTRACT THE PROPERTY TYPE\n",
    "Selector_Example = \"h1.lbl_titulo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1559396971303,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "-UYR51QwrYWW",
    "outputId": "0f612e51-9415-4c34-a69e-d773acedcc3f"
   },
   "outputs": [],
   "source": [
    "# EXAMPLE CODE, RUN TO CHECK THE EXAMPLE SELECTOR\n",
    "\n",
    "try:\n",
    "    f = open(r'{}/listing.html'.format(_dir), \"r\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    f = open(r'listing.html', \"r\")\n",
    "\n",
    "page = f.read()\n",
    "f.close()\n",
    "\n",
    "#parsing HTML into a tree structure\n",
    "tree = html.fromstring(page)\n",
    "\n",
    "print('Example -> Property type: {}'.format(tree.cssselect(Selector_Example)[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7MJswrmP89Z"
   },
   "source": [
    "Now that you understand the example, just fill the CSS selectors here and check it by running the below cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-2ra_gioTOy"
   },
   "outputs": [],
   "source": [
    "############## Q1 ANSWERS ##################\n",
    "\n",
    "'''\n",
    "Gui Palazzo Note:\n",
    "With this selector we can get all data needed in a list of web tag elements. Afterwards, let's use a hash map to get the info we need.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "only_li_elements = \"ul.bloco-dados li\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMYphPt_JeEQ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Gui Palazzo Note:\n",
    "  Section 1: using cssselect to get all li elements in order to extract 4 out of 7 needed informations\n",
    "    to answer the question below.\n",
    "\n",
    "  Section 2: using xpath to get other 3 needed infos.\n",
    "'''\n",
    "\n",
    "\n",
    "#Section 1\n",
    "_dict = {}\n",
    "all_li_elem = tree.cssselect(only_li_elements)\n",
    "\n",
    "for elem in all_li_elem:\n",
    "    text = elem.text_content().strip()\n",
    "    text_splitted = text.split(\": \")\n",
    "    _dict[text_splitted[0]] = text_splitted[1]\n",
    "\n",
    "\n",
    "#Section 2\n",
    "desc = tree.xpath(\"/html/body/div/div[2]/div/div[4]/p/text()\")[0]\n",
    "location = tree.xpath(\"//*[@id='Cpl_lbl_morada']\")[0].text\n",
    "agent_name = tree.xpath(\"//*[@id='Cpl_moduloinformacaolateral_module_holder']/div/div/div[1]\")[0].text[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1559396975660,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "AaJuBU1nqsub",
    "outputId": "021d698d-5324-41f1-c703-0bb5d2587e12"
   },
   "outputs": [],
   "source": [
    "############### RUN TO CHECK YOUR ANSWERS ##################\n",
    "\n",
    "'''\n",
    "Gui Palazzo Note: \n",
    "Apparently there's something wrong with the Total Area in the website (listing.html file), because if \n",
    "  the Living Area is 80m2, how could the Total Area be less than that (0m2)?\n",
    "'''\n",
    "\n",
    "\n",
    "print('Bathrooms: {}'.format(_dict['Bathrooms']))\n",
    "print('')\n",
    "print('Bedrooms: {}'.format(_dict['Bedrooms']))\n",
    "print('')\n",
    "print('Total area: {}'.format(_dict['Total Area']))\n",
    "print('')\n",
    "print('Living area: {}'.format(_dict['Living Area']))\n",
    "print('')\n",
    "print('Description: {}'.format(str(desc)))\n",
    "print('')\n",
    "print('Agent name: {}'.format(agent_name))\n",
    "print('')\n",
    "print('Location: {}'.format(location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__U3ndDeRbt6"
   },
   "source": [
    "__Extra Challenge__:\n",
    "\n",
    "Write from scratch a script to extract and print:\n",
    "* One link that leads to http://mydomain.com/link-to-image\n",
    "* Extract all the features of the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1559396978385,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "iLmuSkUFR-LA",
    "outputId": "72e161af-cb2c-431f-bdc3-948f862d4925"
   },
   "outputs": [],
   "source": [
    "############### WRITE THE SCRIPT TO SOLVE THE EXTRA CHALLENGE HERE ##################\n",
    "\n",
    "try:\n",
    "    f = open(r'{}/listing.html'.format(_dir), \"r\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    f = open(r'listing.html', \"r\")\n",
    "\n",
    "page = f.read()\n",
    "f.close()\n",
    "\n",
    "#parsing HTML into a tree structure\n",
    "tree = html.fromstring(page)\n",
    "\n",
    "\n",
    "def get_link():\n",
    "\n",
    "    xpath = \"/html/body/div/div[2]/div/div[2]/div/a/@href\"\n",
    "    link = str(tree.xpath(xpath)[0])\n",
    "    return link\n",
    "\n",
    "\n",
    "def get_features():\n",
    "  \n",
    "    selector = \"ul.modulo-caracteristicas-conteudo li\"\n",
    "    features = tree.cssselect(selector)\n",
    "    feat_list = [feat.text_content().strip() for feat in features]\n",
    "    return feat_list\n",
    "\n",
    "  \n",
    "print(\"The link is the following: {}\".format(get_link()))\n",
    "print()\n",
    "print(\"The features are the following: {}\".format(get_features()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CX07lJog1Jv"
   },
   "source": [
    "__(2)__ In the second part you will still have to use the html file. However, this time, you should use regular expressions to extract the following data from the webpage:\n",
    "\n",
    "* Urls that are links to listings (i.e.: http://mydomain.com/link-to-listing). Do not use the whole url itself in regular expression. It should select only 3 links.\n",
    "* The agent telephone number\n",
    "* The property price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANpQ4SvPSvpg"
   },
   "outputs": [],
   "source": [
    "# REGEXP EXAMPLE TO EXTRACT THE AGENT EMAIL\n",
    "Regexp_Example = r\"\\\">(.*?@.*?)<\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 727,
     "status": "ok",
     "timestamp": 1559396982986,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "6mPc9TCF2jOx",
    "outputId": "016330fb-f81a-45cb-feb0-9838aa2b3467"
   },
   "outputs": [],
   "source": [
    "# RUN TO CHECK THE EXAMPLE RESULTS\n",
    "\n",
    "try:\n",
    "    f = open(r'{}/listing.html'.format(_dir), \"r\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    f = open(r'listing.html', \"r\")\n",
    "\n",
    "page = f.read()\n",
    "f.close()\n",
    "\n",
    "print(\"Email extracted: {}\".format(re.findall(Regexp_Example, page)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6z_yGAIm8uyC"
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR REGULAR EXPRESSIONS HERE\n",
    "Regexp_1 = r\"http://.*link-to-listing\"\n",
    "Regexp_2 = r\"\\d+(?:-)\\d+\"\n",
    "Regexp_3 = r\"\\d.*\\s€\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1559396985614,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "nRZMlLL8r0Pc",
    "outputId": "2da41de9-d20a-4863-a910-73a4bf92b753"
   },
   "outputs": [],
   "source": [
    "############### RUN TO CHECK YOUR ANSWERS ##################\n",
    "print('Links extrated:')\n",
    "for w in re.findall(Regexp_1, page):\n",
    "    print(w)\n",
    "    \n",
    "print('')\n",
    "print(\"Agent Phone Number: {}\".format(re.findall(Regexp_2, page)[0]))\n",
    "print('')\n",
    "print(\"Property price: {}\".format(re.findall(Regexp_3, page)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4DbKPVDSk8w"
   },
   "source": [
    "__Extra Challenge__\n",
    "* Extract latitude and longitude value from html ()_those values are in the html code, but are not shown on the page__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1559397001965,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "o5AylkERSo6F",
    "outputId": "f3f2e37e-f96d-474d-abcf-61cc63298c2a"
   },
   "outputs": [],
   "source": [
    "############### WRITE THE SCRIPT TO SOLVE THE EXTRA CHALLENGE HERE ##################\n",
    "\n",
    "'''\n",
    "Gui Palazzo Note: \n",
    "\n",
    "  Regex pattern explanation\n",
    "  \n",
    "    -?: matches the minus signal (-) 0 or 1 times\n",
    "    \\d+: matches any digit from 1 to infinite -- similar to \\d{1,}\n",
    "    ?:\\.: try to match a dot (.)\n",
    "    (?:\\.\\d+): this whole submatch consists in a non-capturing group, because we can have natural numbers as latitude and/or longitude\n",
    "                meaning that a dot followed by numbers don't necessarily exists.\n",
    "'''\n",
    "\n",
    "\n",
    "regex_pattern = \"-?\\d+(?:\\.\\d+),-?\\d+(?:\\.\\d+)\"\n",
    "print(re.findall(regex_pattern, page)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6n6ZF7VahF3"
   },
   "source": [
    "# Data Analysis (Python)\n",
    "\n",
    "\n",
    "You obtained all the data that you need and you now need to run an analysis on the following problem. For this part, feel free to use as many cells as you need below this point. Please use properties.csv as your data source.\n",
    "\n",
    "\n",
    "\n",
    "## Problem \n",
    "A private investor is planning an investment in one of the four locations. In order to decide where to invest he needs to know the price impact of such features as ‘pool’, ‘sea view’ and ‘garage’ on properties in each location.\n",
    "He also asks for the mean price of the properties in each type group (‘apartments’, ‘houses’, ‘plots’) and wants to know about properties in the market that are undervalued and overvalued. In order to accomplish the problem that was described we want you to cover the following steps:\n",
    "\n",
    "###Part 1: Data Cleaning\n",
    "As you have seen previously, a lot of information is present in the title/features fields. From there, we want to extract the relevant information for further analysis, such as:\n",
    " - 1A: Property  **type** (as presented in **Details** above) of each property from **title** field\n",
    " - 1B: Property **location** (as presented in **Details** above) of each property from **title** field\n",
    " - 1C: From **features** field, if a property has:\n",
    "  - a pool\n",
    "  - a garage\n",
    "  - sea view\n",
    "\n",
    "####Deliverables part 1:\n",
    "- Create a property dataset with the following schema and save it in a csv file:\n",
    "  - id; location name; type; title; features; pool (0/1); sea view (0/1); garage (0/1);\n",
    "  - pool, sea view, garage should be binary - 1 if the property has the feature and 0 if not\n",
    "- For each of the 3 tasks (1A, 1B, 1C), describe in detail the what you did. What are the advantages and disadvantages of your approach?\n",
    "-  Please provide your code in the cells below, in a reproducible and understandable way;\n",
    "\n",
    "###Part 2: Identify outliers\n",
    "Now that the data is structured correctly, let's look at which properties are a  good deal for our investor. For this you will need to identify undervalued, overvalued, and normal properties in the dataset. Please use any model you find appropiate in order to obtain this.\n",
    "####Deliverables part 2:\n",
    "- As before, deliver a csv file with the following format:\n",
    "  - id; location name; type; area; price; **over-valued (0/1), under-valued (0/1), normal (0/1)**\n",
    " - the new columns should be binary, where for example **over-valued** column would get value 1 if the property is indeed over-valued, 0 otherwise;\n",
    "- A short report (could be a pdf file or new cells within the notebook) containing:\n",
    "  - visualizations (such as scatter plots) discriminating between the undervalued, overvalued and normal properties;\n",
    "  - a explanation of what is the difference between under-valued/over-valued properties and pure data outliers;\n",
    "  - any notes/conclusions you wish to add;\n",
    "- Your code, in the cells below;\n",
    "\n",
    "###Part 3: Theoretical questions\n",
    "- Mention at least 2 hidden traps you found while solving the problems and what would help you to clean the data set;\n",
    "- Describe in detail how you would evaluate the price impact of features such as sea view, pool and garage considering the dataset provided. Your answer should also include how would you deal with missing values, outliers and duplicated listings (same property listing published by different agencies);\n",
    "\n",
    "####Extra challenge:\n",
    "- Describe how would you model the data over time (using createdAt field). What changes over time would you look for and what would you expect the outcomes to be? (i.e. in terms of pricing per location/type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOUPzNkHelpx"
   },
   "source": [
    "#**Data Analysis (Python) - Resolution Starts Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 917,
     "status": "ok",
     "timestamp": 1559406820344,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "n_8QNKbyYET0",
    "outputId": "a34eb5fb-6008-41c4-c86c-cd555edb84b4"
   },
   "outputs": [],
   "source": [
    "# Importing the csv file and creating a dataframe\n",
    "\n",
    "try:\n",
    "    df_original = pd.read_csv('{}/properties.csv'.format(_dir))\n",
    "except: \n",
    "    df_original = pd.read_csv('properties.csv')\n",
    "\n",
    "df = df_original.copy()\n",
    "\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7RR9s5LEc5B"
   },
   "outputs": [],
   "source": [
    "# Showing more rows of the dataframe in order to analyze some columns by looking at the data.\n",
    "\n",
    "pd.set_option('display.max_rows', 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeS2xdi3FAO5"
   },
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcLmw9Yydd6Q"
   },
   "source": [
    "**Creating the words dataframe to use in Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2460,
     "status": "ok",
     "timestamp": 1559399859523,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "GNNDo52ddcm7",
    "outputId": "50cdf7a9-0539-475e-a04b-62ca77b27b6d"
   },
   "outputs": [],
   "source": [
    "# Using nltk in order to create a DataFrame with words I will analyze in the next step\n",
    "\n",
    "stoplist = set(stopwords.words('english') + list(punctuation))\n",
    "texts = df.features.str.lower()\n",
    "word_counts = dict(Counter(word_tokenize('\\n'.join(texts))))\n",
    "\n",
    "w_df = pd.DataFrame(word_counts, index=[0])\n",
    "w_df = w_df.T.reset_index()\n",
    "w_df.rename(columns={0: 'count_words', 'index': 'words'}, inplace=True)\n",
    "\n",
    "w_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t037uqMEbprw"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "*Exploring the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1559397012303,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "AO3PRVlAciYp",
    "outputId": "8e7ef7f9-0850-4d28-b71c-d094a1f40cfe"
   },
   "outputs": [],
   "source": [
    "# Exploring the dataset shape in order to make sure I'm losing the correct amount of lines in case there's a dropna usage or any other methods\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1559397013901,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "naBxLhxtLajF",
    "outputId": "2f3b425a-e243-4a6f-bf04-f36bbca9c746"
   },
   "outputs": [],
   "source": [
    "# Looking at the data types and non-values\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1559397015203,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "9wZnBj3Db3gG",
    "outputId": "5f164c49-cfaf-4f03-aa47-12af9c1f1102"
   },
   "outputs": [],
   "source": [
    "# Exploring the columns names\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36VbwoQPZt5V"
   },
   "source": [
    "**Investigation for Part 1A (Property Type)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 910,
     "status": "ok",
     "timestamp": 1559420406323,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "yyMHC3J4Z-2l",
    "outputId": "4b623e46-bb10-4ddd-9533-76b94aca1c88"
   },
   "outputs": [],
   "source": [
    "df_aux = df[['id', 'title']]\n",
    "\n",
    "\n",
    "#See why the following regex_patterns are as comprehensive as they need to be in the Section below named: \n",
    "  #\"RegEx section validation for Property type\"\n",
    "regex_pattern1 = r\"apartments?\"\n",
    "regex_pattern2 = r\"houses?\"\n",
    "regex_pattern3 = r\"plots?\"\n",
    "\n",
    "\n",
    "df_aux['type'] = df_aux.title.apply(lambda x: 'apartments' if re.search(regex_pattern1, x.lower()) is not None else\n",
    "                                              'houses' if re.search(regex_pattern2, x.lower()) is not None else\n",
    "                                              'plots' if re.search(regex_pattern3, x.lower()) is not None else\n",
    "                                              'unknown')\n",
    "\n",
    "\n",
    "#This is going to be joined to generate the final_df\n",
    "df_id_type = df_aux[['id', 'type']]\n",
    "df_id_type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxMDm2fUtVv7"
   },
   "source": [
    "**RegEx section validation for Property type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1559420864001,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "_gedPF1Jtg5a",
    "outputId": "2c3276c8-8794-4181-e0d6-c500db30bac0"
   },
   "outputs": [],
   "source": [
    "# Testing all the ways \"apartments\" could be written\n",
    "\n",
    "# regex_pattern = r\"ap(?:[a-z])\"  #since there's no \"ap\" word, it seems that there's no \"ap\" word referring to \"apartment\"\n",
    "  #havind said that, we can try to match apartment or apartments words\n",
    "\n",
    "regex_pattern = r\"apartments?\"\n",
    "list_to_search = w_df.words.str.cat(sep=', ').lower()\n",
    "\n",
    "#I would maintain this as a np.array, but since all the words_list variables are list, I'm going to maintain the same pattern\n",
    "apartments_words_list = list(np.unique(np.array(re.findall(regex_pattern, list_to_search))))\n",
    "apartments_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1559420864003,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "EUP8NKaMtAoZ",
    "outputId": "326b9bb0-5615-4417-d421-3ce79b27a206"
   },
   "outputs": [],
   "source": [
    "# Testing all the ways \"houses\" could be written\n",
    "\n",
    "# patterns tested: houses?, home?, household?, homestead?, homebase?, home\\sbase?\n",
    "# there are others ways to refer to a house, like \"residence\" and so on. But since the problem statement says to separate only \"houses\"\n",
    "  #I'll keep with that for now\n",
    "\n",
    "regex_pattern = r\"houses?\"\n",
    "list_to_search = w_df.words.str.cat(sep=', ').lower()\n",
    "\n",
    "#I would maintain this as a np.array, but since all the words_list variables are list, I'm going to maintain the same pattern\n",
    "houses_words_list = list(np.unique(np.array(re.findall(regex_pattern, list_to_search))))\n",
    "houses_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1559420864378,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "7UT5qkJltAsD",
    "outputId": "963787e6-4b52-4d23-e060-bb63e1440db6"
   },
   "outputs": [],
   "source": [
    "# Testing all the ways \"plots\" could be written\n",
    "  \n",
    "regex_pattern = r\"plots?\"\n",
    "list_to_search = w_df.words.str.cat(sep=', ').lower()\n",
    "\n",
    "#I would maintain this as a np.array, but since all the words_list variables are list, I'm going to maintain the same pattern\n",
    "plots_words_list = list(np.unique(np.array(re.findall(regex_pattern, list_to_search))))\n",
    "plots_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roXd-ruUu9vy"
   },
   "source": [
    "**END OF RegEx section validation for Property type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJt-cqpRsR7f"
   },
   "source": [
    "**Investigation for Part 1B (Property Location)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1559403656047,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "n0_u8YucZ_BL",
    "outputId": "22d4eab1-b4e8-4c7b-a1c2-b8d790820312"
   },
   "outputs": [],
   "source": [
    "wdf_aux_1b = w_df.copy()\n",
    "wdf_aux_1b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2205,
     "status": "ok",
     "timestamp": 1559424382335,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "nYcKYa6bs3vh",
    "outputId": "d44d5193-2dac-4dbc-fd27-fdf07172ef8a"
   },
   "outputs": [],
   "source": [
    "#there are many location, such as cities, beaches and so on\n",
    "#there seems to have an overlapping of beaches in cities and sometimes it brings only the city instead of the beach.\n",
    "#for this analysis, to avoid a lot of complexity (given the time), I chose some names as a sample.\n",
    "\n",
    "#the names can be found in the section below where I filter them.\n",
    "\n",
    "#GPE label is specific for geographical data (cities, coutries, ...)\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "_list = []\n",
    "\n",
    "for i, _title in enumerate(df.title):\n",
    "    _title = _title.strip()\n",
    "    doc = nlp(_title)\n",
    "    print([X.text for X in doc.ents if X.label_ == 'GPE'])\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 723,
     "status": "ok",
     "timestamp": 1559424962472,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "BbyoUgh08ekk",
    "outputId": "a9831256-33a6-4729-99ee-f2066a2b8b0f"
   },
   "outputs": [],
   "source": [
    "df_aux = df[['id', 'title']]\n",
    "\n",
    "regex_pattern1 = r\"costa nagüeles III\"\n",
    "regex_pattern2 = r\"las cañas beach\"\n",
    "regex_pattern3 = r\"nagüeles\"\n",
    "regex_pattern4 = r\"montepiedra\"\n",
    "regex_pattern5 = r\"andalucia\"\n",
    "regex_pattern6 = r\"alhambra del mar\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_aux['location'] = df_aux.title.apply(lambda x: 'costa_nagueles_III' if re.search(regex_pattern1, x.lower()) is not None else\n",
    "                                                  'las_canas_beach' if re.search(regex_pattern2, x.lower()) is not None else\n",
    "                                                  'nagueles' if re.search(regex_pattern3, x.lower()) is not None else\n",
    "                                                  'montepiedra' if re.search(regex_pattern4, x.lower()) is not None else\n",
    "                                                  'andalucia' if re.search(regex_pattern5, x.lower()) is not None else\n",
    "                                                  'alhambra_del_mar' if re.search(regex_pattern6, x.lower()) is not None else\n",
    "                                                  'unknown')\n",
    "\n",
    "df_aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 676,
     "status": "ok",
     "timestamp": 1559424990257,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "cdti2baX-DT_",
    "outputId": "59a64526-701f-489b-a0b6-1788b67fce21"
   },
   "outputs": [],
   "source": [
    "#Evaluating the amount of data I'm losing by selecting only this sample\n",
    "\n",
    "#I'm working with ~53% of the data\n",
    "df_aux.groupby('location')['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1559425126912,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "0jNt_c7p-smx",
    "outputId": "0f4e3b46-b498-48ab-f57b-d9773ebc1a81"
   },
   "outputs": [],
   "source": [
    "#This is going to be joined to generate the final_df\n",
    "df_id_location = df_aux[['id', 'location']]\n",
    "df_id_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-jYaa7KY7Cb"
   },
   "source": [
    "**Investigation for Part 1C (Property Features: pool, garage and sea view)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1559415074896,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "HCmlo-7-cBlx",
    "outputId": "fc756627-420a-413c-ada6-69d771fdbe4f"
   },
   "outputs": [],
   "source": [
    "# Filling nan values with Unknown in order to display only data that means something\n",
    "df.features.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "\n",
    "# Giving a quick look in the way data is written, how are some exceptions I must handle in order to extract the info I need\n",
    "for i, feat in enumerate(df.features):\n",
    "    print(i, \"--\", feat)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1559421560852,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "90YfUhpCxE0n",
    "outputId": "1bf01b81-5c74-4468-b492-ce97cdf01a5a"
   },
   "outputs": [],
   "source": [
    "df_aux = df[['id', 'features']]\n",
    "df_aux.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wpDiVYg1J3S"
   },
   "source": [
    "**RegEx section validation for Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ep8QNED91is-"
   },
   "outputs": [],
   "source": [
    "# Creating the columns I'll use to further analysis\n",
    "\n",
    "wdf_aux_1c = w_df.copy()\n",
    "\n",
    "wdf_aux_1c['if_pool'] = wdf_aux_1c.words.apply(lambda x: True if 'pool' in x.lower() else False)\n",
    "wdf_aux_1c['if_garage'] = wdf_aux_1c.words.apply(lambda x: True if 'garage' in x.lower() else False)\n",
    "wdf_aux_1c['if_seaView'] = wdf_aux_1c.words.apply(lambda x: True if 'sea' in x.lower() else False)\n",
    "\n",
    "df_aux = wdf_aux_1c.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1559423108687,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "nn7aBq0c1pwc",
    "outputId": "3f538357-3fc6-4a75-efaf-710ea9c28e4b"
   },
   "outputs": [],
   "source": [
    "# Printing words related to 'pool' to understand if there's anything I must avoid\n",
    "\n",
    "_df_aux = df_aux[df_aux.if_pool]\n",
    "print(_df_aux)\n",
    "\n",
    "# Analyzing the df below, the following snippet contains must-have words related to 'pool'\n",
    "\n",
    "pool_word_list = list(_df_aux[~_df_aux.words.isin(['whirlpool'])].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1559423109012,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "5kk0sJtw1wrp",
    "outputId": "c81f74a1-3cdd-48a7-be4d-62a8d11aeacc"
   },
   "outputs": [],
   "source": [
    "# Printing words related to 'garage' to understand if there's anything I must avoid\n",
    "\n",
    "_df_aux = df_aux[df_aux.if_garage]\n",
    "print(_df_aux)\n",
    "\n",
    "# Analyzing the df below, the following snippet contains must-have words to 'garage'\n",
    "\n",
    "garage_word_list = list(_df_aux.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1559423109014,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "n7rnSvVj10zW",
    "outputId": "bf9ba70b-3964-426c-cc44-cba3c8dbd190"
   },
   "outputs": [],
   "source": [
    "# Printing words related to 'sea view' to understand if there's anything I must avoid\n",
    "\n",
    "_df_aux = df_aux[df_aux.if_seaView]\n",
    "print(_df_aux)\n",
    "\n",
    "# Analyzing the df below, the following snippet contains must-have words to 'sea view'\n",
    "\n",
    "seaView_word_list = list(_df_aux[_df_aux.words.isin(['sea/lake', 'sea', 'sea/beach', 'seaside'])].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 652,
     "status": "ok",
     "timestamp": 1559423118887,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "yEnBXucs137r",
    "outputId": "1997145c-cd52-4ed9-bcd2-a38893958b69"
   },
   "outputs": [],
   "source": [
    "# Since 'sea view' is a compound word, I could be losing data in the way words are been counted\n",
    "  # so let's try to quantify this loss\n",
    "\n",
    "wdf_aux_1c['if_seaView_viewTest'] = wdf_aux_1c.words.apply(lambda x: True if 'view' in x.lower() else False)\n",
    "wdf_aux_1c['if_seaView_seaTest'] = wdf_aux_1c.words.apply(lambda x: True if 'sea' in x.lower() else False)\n",
    "\n",
    "df_aux = wdf_aux_1c.copy()\n",
    "\n",
    "_df_aux = df_aux[df_aux.if_seaView_viewTest]\n",
    "print(\"View Words DataFrame Analysis\")\n",
    "print(_df_aux)\n",
    "\n",
    "\n",
    "_df_aux = df_aux[df_aux.if_seaView_seaTest]\n",
    "print(\"\\n\\nSea Words DataFrame Analysis\")\n",
    "print(_df_aux)\n",
    "\n",
    "\n",
    "# According to the \"View Words DataFrame Analysis\", even if all the words preceding 'view(s)' were sea/lake or any 'sea' word \n",
    "# related, the maximum 'sea view(s)' I could get is 598 + 451 = 1049\n",
    "\n",
    "# For words that make sense to be related to 'Sea view', I would choose: sea/lake, sea, sea/beach, seaside\n",
    "# Having said that, I could have: 26 + 941 + 40 + 3 = 1010\n",
    "\n",
    "# So, the loss would be: (1049-1010) / 1049 = 3.72%\n",
    "# Even if every 'view(s)' word are related to 'sea' words, I would be losing 3.72% of the data and I'm ok with that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 646,
     "status": "ok",
     "timestamp": 1559423122097,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "Ah-CfbTW18Kk",
    "outputId": "bb8aeaec-4f6a-4b0c-d6ca-665e5d76dc37"
   },
   "outputs": [],
   "source": [
    "#only for printing purposes and understanding, this is the whole dataframe with all the booleans tests\n",
    "\n",
    "wdf_aux_1c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw__4ctA1_cd"
   },
   "source": [
    "**End of RegEx section validation for Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1678,
     "status": "ok",
     "timestamp": 1559423194256,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "sQdt6NmKxLve",
    "outputId": "737bbac8-8a8c-4826-b4b4-0bbb444234aa"
   },
   "outputs": [],
   "source": [
    "df_aux = df[['id', 'features']]\n",
    "\n",
    "df_aux['pool'] = df_aux.features.apply(lambda x: 1 if re.search(\"(?!.*whirl)pool\", x.strip().lower()) is not None else 0)\n",
    "df_aux['garage'] = df_aux.features.apply(lambda x: 1 if re.search(\"garage\", x.strip().lower()) is not None else 0)\n",
    "df_aux['sea_view'] = df_aux.features.apply(lambda x: 1 if re.search(\"(?!.*ba)sea(?![p, t, r])\", x.strip().lower()) is not None else 0)\n",
    "\n",
    "df_aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1559423351990,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "KKhluWrt3cm-",
    "outputId": "ba729900-2fa0-419f-9dc1-f122713282d8"
   },
   "outputs": [],
   "source": [
    "#This is going to be joined to generate the final_df\n",
    "\n",
    "df_id_features = df_aux[['id', 'pool', 'garage', 'sea_view']]\n",
    "df_id_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1559425458535,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 180
    },
    "id": "4GABTXUfsp8I",
    "outputId": "2283f663-8119-4877-ad50-80ebe04d825a"
   },
   "outputs": [],
   "source": [
    "# Deliverable Part 1\n",
    "\n",
    "#Since the final_df is getting data directly from the original df, I wanna replicate all the data from it,\n",
    "  #that's why I'm doing a left join on the other dfs.\n",
    "\n",
    "final_df = df[['id', 'title', 'features']]\n",
    "final_df = final_df.merge(df_id_type, how='left', on='id')\n",
    "final_df = final_df.merge(df_id_features, how='left', on='id')\n",
    "final_df = final_df.merge(df_id_location, how='left', on='id')\n",
    "\n",
    "final_df = final_df[['id', 'location', 'type', 'title', 'features', 'pool', 'sea_view', 'garage']]\n",
    "final_df.head()\n",
    "\n",
    "\n",
    "'''\n",
    "** Comments\n",
    "\n",
    " All this analysis requires a much more detailed approach on which location to consider: city, beach, hotels name? I did a simplified one\n",
    "    by getting only a sample; If we decide to go with more granular data, like beach, we must understand the overlapping between beaches, and\n",
    "    a good approach for that is cascading geographical data upstream (country > city > beach > hotels).\n",
    " \n",
    " Moreover, some compound words like 'sea view' are not 100% matched since 'sea' counts as one word separetely from 'view',\n",
    "    and therefore I can't ensure it's totally correct.\n",
    " \n",
    " The way I created the dfs to be joined are not best one actually, because there were only few variables, but imagine a scenario\n",
    "    where there are 100 variables -- I would had to write 100 lines one by one.\n",
    " \n",
    " All in all, given the scenario, I would stick with this approach to simplify the things, and because I think it gives a good\n",
    "    view of my coding skills.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jbigs5OEmkbl"
   },
   "outputs": [],
   "source": [
    "# Creating the csv\n",
    "\n",
    "final_df.to_csv('df_part1.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Data_Analytics_Ironhack_TA_SP.ipynb",
   "provenance": [
    {
     "file_id": "1tXFbnSJaYeofrqCsTFInzRgFWcK7p3TH",
     "timestamp": 1559429908600
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
